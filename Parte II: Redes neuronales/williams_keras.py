# -*- coding: utf-8 -*-
"""WILLIAMS_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wamswGNfQX_g2cUaV36WL8gf7l6O_dvt
"""

!pip install ta

# Commented out IPython magic to ensure Python compatibility.
import tensorflow.keras as kr
import tensorflow
# %tensorflow_version
import numpy as np
import matplotlib.pyplot as plt
import ta
import pandas as pd
import pandas_datareader.data as web
import datetime as dt

williams_df = pd.read_excel('williams.xlsx')
output_df = pd.read_excel('output_w.xlsx')
williams_test_df = pd.read_excel('williams_test.xlsx')

matriz_williams = np.transpose(williams_df.to_numpy())
matriz_output = output_df.to_numpy()
matriz_williams_test = np.transpose(williams_test_df.to_numpy())

williams_data = matriz_williams[:,:20]
output_data = matriz_output[19:,:]
williams_test = matriz_williams_test[:,:20]

for i in range(matriz_williams.shape[1]-20):
    williams_data = np.concatenate((williams_data, matriz_williams[:,(i+1):(i+21)]))

for j in range(matriz_williams_test.shape[1]-20):
        williams_test = np.concatenate((williams_test, matriz_williams_test[:,(j+1):(j+21)]))

# Datos de entrenamiento
X = williams_data
Y = output_data
 
# Datos de prueba
X_test = williams_test

lr = 0.01
nn = [1, 20, 20, 1]

# Creamos el objeto que contendrá a nuestra red neuronal, como
# secuencia de capas.
model = kr.Sequential()

# Añadimos la capa 1
l1 = model.add(kr.layers.Dense(nn[1], activation='relu'))

# Añadimos la capa 2
l2 = model.add(kr.layers.Dense(nn[2], activation='relu'))

# Añadimos la capa 3
l3 = model.add(kr.layers.Dense(nn[3], activation='sigmoid'))

# Compilamos el modelo, definiendo la función de coste y el optimizador.
model.compile(loss='mse', optimizer=kr.optimizers.SGD(lr=lr), metrics=['acc'])

# Y entrenamos al modelo.
history = model.fit(X, Y, batch_size=32, epochs=200)

# Gráficos
loss_train = history.history['loss']
epoch_count = range(1, len(loss_train) + 1)
plt.plot(epoch_count, loss_train, 'g', label='Training loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss_train = history.history['acc']
epoch_count = range(1, len(loss_train) + 1)
plt.plot(epoch_count, loss_train, 'g', label='Training accuracy')
plt.title('Training accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

predicciones = model.predict(X_test)
predicciones = pd.DataFrame({'probabilidades': predicciones[:,0]} )

ptos_compra = predicciones.where(predicciones['probabilidades']<0.1)
ptos_venta = predicciones.where(predicciones['probabilidades']>0.9)

ptos_compra.to_excel('ptos_compra_williams.xlsx')
ptos_venta.to_excel('ptos_venta_williams.xlsx')